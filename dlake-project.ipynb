{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, from_unixtime\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, dayofweek, date_format\n",
    "from pyspark.sql.types import StructType, StructField as Fld, DoubleType as Dbl,StringType as Str, IntegerType as Int, DateType as Date,TimestampType as Ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.session.Session(region_name='us-east-1')\n",
    "s3 = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    Description: Creates spark session.\n",
    "    Returns:\n",
    "        spark session object\n",
    "    \"\"\"\n",
    "#     AWS_ACCESS_KEY_ID = os.environ['AWS_ACCESS_KEY_ID']\n",
    "#     AWS_SECRET_ACCESS_KEY = os.environ['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    import pdb;pdb.set_trace()\n",
    "\n",
    "#     spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", AWS_ACCESS_KEY_ID)\n",
    "#     spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY)\n",
    "#     spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3n.awsAccessKeyId\", AWS_ACCESS_KEY_ID)\n",
    "#     spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3n.awsSecretAccessKey\", AWS_SECRET_ACCESS_KEY)\n",
    "#     spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "#     spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3n.endpoint\", \"s3.amazonaws.com\")\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def song_schema():\n",
    "    \"\"\"\n",
    "    Description: Provides the schema for the staging_songs table.\n",
    "    Returns:\n",
    "        spark dataframe schema object\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        Fld(\"num_songs\", Int()),\n",
    "        Fld(\"artist_id\", Str()),\n",
    "        Fld(\"artist_latitude\", Dbl()),\n",
    "        Fld(\"artist_longitude\", Dbl()),\n",
    "        Fld(\"artist_location\", Str()),\n",
    "        Fld(\"artist_name\", Str()),\n",
    "        Fld(\"song_id\", Str()),\n",
    "        Fld(\"title\", Str()),\n",
    "        Fld(\"duration\", Dbl()),\n",
    "        Fld(\"year\", Int())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_song_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Description: Read in songs data from json files.\n",
    "                 Outputs songs and artists dimension tables in parquet files in S3.\n",
    "    Arguments:\n",
    "        spark: the spark session object. \n",
    "        input_data: path to the S3 bucket containing input json files.\n",
    "        output_data: path to S3 bucket that will contain output parquet files. \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # get filepath to song data file\n",
    "    song_data = input_data + 'song_data/*/*/*/*.json'\n",
    "    \n",
    "    import pdb;pdb.set_trace()\n",
    "    \n",
    "    # read song data file\n",
    "    df = spark.read.json(song_data, schema=song_schema())\n",
    "    \n",
    "    import pdb;pdb.set_trace()\n",
    "\n",
    "    # extract columns to create songs table\n",
    "    songs_table = df.select(['song_id', 'title', 'artist_id', \n",
    "                             'year', 'duration']).distinct().where(\n",
    "                             col('song_id').isNotNull())\n",
    "    \n",
    "    import pdb;pdb.set_trace()\n",
    "    \n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    songs_path = output_data + 'songs'\n",
    "    songs_table.write.partitionBy('year', 'artist_id').parquet(songs_path)\n",
    "    \n",
    "    import pdb;pdb.set_trace()\n",
    "\n",
    "    # extract columns to create artists table\n",
    "    artists_table = df.select(['artist_id', 'artist_name', 'artist_location', \n",
    "                             'artist_latitude', 'artist_longitude']).distinct().where(\n",
    "                             col('artist_id').isNotNull())\n",
    "    \n",
    "    import pdb;pdb.set_trace()\n",
    "    \n",
    "    # write artists table to parquet files\n",
    "    artists_path = output_data + 'artists'\n",
    "    artists_table.write.parquet(artists_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_log_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Description: Read in logs data from json files.\n",
    "                 Outputs time and users dimension tables, songplays fact table\n",
    "                in parquet files in S3.\n",
    "    Arguments:\n",
    "        spark: the spark session object. \n",
    "        input_data: path to the S3 bucket containing input json files.\n",
    "        output_data: path to S3 bucket that will contain output parquet files. \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # get filepath to log data file\n",
    "    log_data = input_data + 'log-data/'\n",
    "\n",
    "    # read log data file\n",
    "    df = spark.read.json(log_data)\n",
    "    \n",
    "    # filter by actions for song plays\n",
    "    df = df.filter(df.page == 'NextSong')\n",
    "\n",
    "    # extract columns for users table\n",
    "    users_table = df.select(['userId', 'firstName', 'lastName', \n",
    "                             'gender', 'level']).distinct().where(\n",
    "                             col('userId').isNotNull())\n",
    "    \n",
    "    # write users table to parquet files\n",
    "    users_path = output_data + 'users'\n",
    "    users_table.write.parquet(users_path)\n",
    "    \n",
    "    def format_datetime(ts):\n",
    "        \"\"\"\n",
    "        Description: converts numeric timestamp to datetime format.\n",
    "        Returns:\n",
    "            timestamp with type datetime\n",
    "        \"\"\"\n",
    "        return datetime.fromtimestamp(ts/1000.0)\n",
    "\n",
    "    # create timestamp column from original timestamp column\n",
    "    get_timestamp = udf(lambda x: format_datetime(int(x)), Ts())\n",
    "    df = df.withColumn(\"start_time\", get_timestamp(df.ts))\n",
    "    \n",
    "    # create datetime column from original timestamp column\n",
    "    get_datetime = udf(lambda x: format_datetime(int(x)), Date())\n",
    "    df = df.withColumn(\"datetime\", get_datetime(df.ts))\n",
    "    \n",
    "    # extract columns to create time table\n",
    "    time_table = df.select('ts', 'start_time', 'datetime',\n",
    "                           hour(\"datetime\").alias('hour'),\n",
    "                           dayofmonth(\"datetime\").alias('day'),\n",
    "                           weekofyear(\"datetime\").alias('week'),\n",
    "                           year(\"datetime\").alias('year'),\n",
    "                           month(\"datetime\").alias('month'),\n",
    "                           dayofweek(\"datetime\").alias('weekday')\n",
    "                          ).dropDuplicates()\n",
    "    \n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    time_table_path = output_data + 'time'\n",
    "    time_table.write.partitionBy('year', 'month').parquet(time_table_path)\n",
    "\n",
    "    # read in song data to use for songplays table\n",
    "    songs_path = input_data + 'song_data/*/*/*/*.json'\n",
    "    song_df = spark.read.json(songs_path, schema=song_schema())\n",
    "\n",
    "    # extract columns from joined song and log datasets to create songplays table\n",
    "    df = df.drop_duplicates(subset=['start_time'])\n",
    "    songplays_table = song_df.alias('s').join(df.alias('l'), \n",
    "                                             (song_df.title == df.song) & \\\n",
    "                                             (song_df.artist_name == df.artist)).where(\n",
    "                                             df.page == 'NextSong').select([\n",
    "                                             col('l.start_time'),\n",
    "                                             year(\"l.datetime\").alias('year'),\n",
    "                                             month(\"l.datetime\").alias('month'),\n",
    "                                             col('l.userId'),\n",
    "                                             col('l.level'),\n",
    "                                             col('s.song_id'),\n",
    "                                             col('s.artist_id'),\n",
    "                                             col('l.sessionID'),\n",
    "                                             col('l.location'),\n",
    "                                             col('l.userAgent')\n",
    "                                            ])\n",
    "\n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    songplays_path = output_data + 'songplays'\n",
    "    songplays_table.write.partitionBy('year', 'month').parquet(songplays_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-14-c5ef6db56c72>(20)create_spark_session()\n",
      "-> return spark\n",
      "(Pdb) c\n",
      "(Pdb) c\n",
      "> <ipython-input-24-aa1cb054a18d>(11)main()\n",
      "-> input_data = \"s3a://mstar-quant-non-prod-workspaces/viraj/\"\n",
      "> <ipython-input-24-aa1cb054a18d>(17)main()\n",
      "-> process_log_data(spark, input_data, output_data)\n",
      "(Pdb) c\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Description: Calls functions to create spark session, read from S3\n",
    "                 and perform ETL to S3 Data Lake.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    spark = create_spark_session()\n",
    "    \n",
    "    import pdb;pdb.set_trace()\n",
    "    input_data = \"s3a://mstar-quant-non-prod-workspaces/viraj/\"\n",
    "    output_data = \"s3a://mstar-quant-non-prod-workspaces/viraj/op\"\n",
    "    \n",
    "    import pdb;pdb.set_trace()\n",
    "    \n",
    "#     process_song_data(spark, input_data, output_data)    \n",
    "    process_log_data(spark, input_data, output_data)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
